{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3544c8fe-816c-4f4e-a6a8-0f837fad3363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded and Combined\n",
      "                                     Input.audio_url buzz_majority  buzz_mean  \\\n",
      "0  https://patrickchwalek.com/audio/native/mp3/Na...           yes   1.000000   \n",
      "1  https://patrickchwalek.com/audio/native/mp3/Na...           yes   0.666667   \n",
      "2  https://patrickchwalek.com/audio/invasive/mp3/...           yes   0.666667   \n",
      "3  https://patrickchwalek.com/audio/native/mp3/Na...            no   0.333333   \n",
      "4  https://patrickchwalek.com/audio/invasive/mp3/...           yes   1.000000   \n",
      "\n",
      "   0  1  2  3  4  5  6  ...  14  15  16  17  18  sum  MAX_count  1sec_chunk  \\\n",
      "0  0  0  0  3  0  0  0  ...   3   0   0   0   0   13          4           1   \n",
      "1  0  0  0  0  0  0  0  ...   0   0   0   0   0   13          4           1   \n",
      "2  0  0  0  1  0  0  0  ...   3   0   0   0   2   13          4           1   \n",
      "3  0  0  2  1  0  0  1  ...   0   0   0   0   0   13          4           1   \n",
      "4  4  0  0  0  0  0  0  ...   0   0   0   0   0   13          4           1   \n",
      "\n",
      "    19  label  \n",
      "0  0.0      1  \n",
      "1  0.0      1  \n",
      "2  0.0      1  \n",
      "3  0.0      1  \n",
      "4  0.0      1  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Features and Labels Extracted\n",
      "Feature shape: (2418, 23), Label shape: (2418,)\n",
      "Missing values handled\n",
      "Feature shape: (2418, 23), Label shape: (2418,)\n",
      "Data Split into Training and Testing Sets\n",
      "Training set size: 1934, Testing set size: 484\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "positive_df = pd.read_csv('positive_dataset_2.csv')\n",
    "negative_df = pd.read_csv('negative_dataset_2.csv')\n",
    "\n",
    "# Standardize column names\n",
    "positive_df = positive_df.rename(columns={'1sec_chunk': '1sec_chunk'})\n",
    "negative_df = negative_df.rename(columns={'1secondchunk': '1sec_chunk_no_buzz',})\n",
    "\n",
    "# Subtract 19 from each value in the '1sec_chunk_no_buzz' column of the negative_df\n",
    "negative_df['1sec_chunk_no_buzz'] = negative_df['1sec_chunk_no_buzz'] - 19\n",
    "# Rename '1sec_chunk_no_buzz' to '1sec_chunk'\n",
    "negative_df = negative_df.rename(columns={'1sec_chunk_no_buzz': '1sec_chunk'})\n",
    "positive_df['1sec_chunk'] = positive_df['1sec_chunk'].fillna(0)\n",
    "negative_df['1sec_chunk'] = negative_df['1sec_chunk'].fillna(0)\n",
    "if '19' not in positive_df.columns:\n",
    "    positive_df['19'] = 0\n",
    "\n",
    "positive_df['label'] = 1  # Buzz present\n",
    "negative_df['label'] = 0  # No buzz\n",
    "\n",
    "combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "print(\"Data Loaded and Combined\")\n",
    "print(combined_df.head())\n",
    "feature_columns = [str(i) for i in range(19)] + ['1sec_chunk','MAX_count', 'sum']\n",
    "X = combined_df[feature_columns].values\n",
    "y = combined_df['label'].values\n",
    "print(\"Features and Labels Extracted\")\n",
    "print(f\"Feature shape: {X.shape}, Label shape: {y.shape}\")\n",
    "combined_df.fillna(0, inplace=True)\n",
    "X = combined_df[feature_columns].values\n",
    "y = combined_df['label'].values\n",
    "print(\"Missing values handled\")\n",
    "print(f\"Feature shape: {X.shape}, Label shape: {y.shape}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data Split into Training and Testing Sets\")\n",
    "print(f\"Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999310e5-3058-4d9d-89a1-9b1dab9d2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN Model Defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = nn.ConstantPad1d((0, -padding), 0) #trim the padding \n",
    "        self.relu1 = nn.ReLU() #could  consdier using sigmoid or a diffrent activation function\n",
    "        self.dropout1 = nn.Dropout(dropout) #sets random fraction of smaple to 0 for avioding overfitting\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None #match the dimensions of I/O if they are different, enabling residual connections\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01) #followed a normal distribution for initializing weights of  conv and downsample layer\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x): #pass on temporal block\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "        \n",
    "#define architecture of the temporal layer\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)  #number of  temporal blocks\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,  #append new block to layer array\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        return self.network(x)\n",
    "\n",
    "#add a linear layer while using temproal block cearted before\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])\n",
    "\n",
    "print(\"TCN Model Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da7906b-08fa-43f0-a8c8-f55b5f278e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared for PyTorch\n",
      "Train loader length: 61, Test loader length: 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) #we could heighten the batch size based on our  compute resource\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Data Prepared for PyTorch\")\n",
    "print(f\"Train loader length: {len(train_loader)}, Test loader length: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558fd882-f228-4210-b91b-092468b5e476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.2856\n",
      "Epoch 2/20, Loss: 0.0934\n",
      "Epoch 3/20, Loss: 0.0468\n",
      "Epoch 4/20, Loss: 0.0212\n",
      "Epoch 5/20, Loss: 0.0174\n",
      "Epoch 6/20, Loss: 0.0123\n",
      "Epoch 7/20, Loss: 0.0073\n",
      "Epoch 8/20, Loss: 0.0071\n",
      "Epoch 9/20, Loss: 0.0054\n",
      "Epoch 10/20, Loss: 0.0047\n",
      "Epoch 11/20, Loss: 0.0049\n",
      "Epoch 12/20, Loss: 0.0048\n",
      "Epoch 13/20, Loss: 0.0055\n",
      "Epoch 14/20, Loss: 0.0041\n",
      "Epoch 15/20, Loss: 0.0029\n",
      "Epoch 16/20, Loss: 0.0028\n",
      "Epoch 17/20, Loss: 0.0033\n",
      "Epoch 18/20, Loss: 0.0021\n",
      "Epoch 19/20, Loss: 0.0024\n",
      "Epoch 20/20, Loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TCN(input_size=1, output_size=1, num_channels=[25, 25], kernel_size=7, dropout=0.3).to(device) #fine tuning can be done here\n",
    "criterion = nn.BCEWithLogitsLoss() #suitable for binary classification tasks where the output is a probability, predict vs actual binaires\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) #common optimizers for bianry classification problems and parameters, could finetune later\n",
    "\n",
    "num_epochs = 20 \n",
    "best_val_loss = float('inf')\n",
    "early_stop_patience = 5\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device) #this might be where Marco in terms of device setting(not for  training will  use cluster).\n",
    "        optimizer.zero_grad() #clear graduates from tensors\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() \n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d1d868-baba-488b-9327-6489e53546ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0001\n",
      "Model Training Completed\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        val_loss += loss.item()\n",
    "val_loss /= len(test_loader)\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "# Early Stopping for overfitting and generalization\n",
    "if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    early_stop_counter = 0\n",
    "else:\n",
    "    early_stop_counter += 1\n",
    "    if early_stop_counter >= early_stop_patience:\n",
    "        print(\"Early stopping\")\n",
    "\n",
    "print(\"Model Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026ddf7c-336d-4cf0-a49b-fce63a3e5907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad(): #diabsles gradient computation to save memory, may not be needed\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device) #X_batch contains input data, y_batch contains true labels.\n",
    "        outputs = model(X_batch)\n",
    "        test_loss += criterion(outputs.squeeze(), y_batch).item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = correct / len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19066185-d631-4be7-932a-e303000367ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracy: 1.0000\n",
      "Fold Accuracy: 1.0000\n",
      "Fold Accuracy: 1.0000\n",
      "Fold Accuracy: 1.0000\n",
      "Fold Accuracy: 1.0000\n",
      "Mean Cross-Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#haver to see what cross validation is being done\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_accuracies = []\n",
    "\n",
    "# for train_index, val_index in kf.split(X):\n",
    "#     X_train_cv, X_val_cv = X[train_index], X[val_index]\n",
    "#     y_train_cv, y_val_cv = y[train_index], y[val_index]\n",
    "\n",
    "#     X_train_tensor = torch.tensor(X_train_cv, dtype=torch.float32).unsqueeze(1)\n",
    "#     y_train_tensor = torch.tensor(y_train_cv, dtype=torch.float32)\n",
    "\n",
    "#     X_val_tensor = torch.tensor(X_val_cv, dtype=torch.float32).unsqueeze(1)\n",
    "#     y_val_tensor = torch.tensor(y_val_cv, dtype=torch.float32)\n",
    "\n",
    "#     train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#     val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#     model = TCN(input_size=1, output_size=1, num_channels=[25, 25], kernel_size=7, dropout=0.2).to(device)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#     num_epochs = 20\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs.squeeze(), y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in val_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             outputs = model(X_batch)\n",
    "#             predicted = (outputs.squeeze() > 0.5).float()\n",
    "#             correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "#     accuracy = correct / len(val_loader.dataset)\n",
    "#     cv_accuracies.append(accuracy)\n",
    "#     print(f'Fold Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# mean_cv_accuracy = np.mean(cv_accuracies)\n",
    "# print(f'Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb239c-297f-4fc4-92b0-2ea92987a7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
